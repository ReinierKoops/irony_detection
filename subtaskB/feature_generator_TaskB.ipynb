{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script generates the features for classification in subtask B. Set the TRAIN flag to True to \n",
    "    generate features for the training data, and to False to generate features for test data\"\"\"\n",
    "\n",
    "import json, re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "from load import parse_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    dataset='../datasets/train/SemEval2018-T3-train-taskB_emoji.txt'\n",
    "    corpus, _ = parse_dataset(dataset)\n",
    "    corpus_preprocessed = json.load(open('../extra_resources/train_preprocessed.txt','r'))\n",
    "else:\n",
    "    dataset='../datasets/goldtest_TaskB/SemEval2018-T3_gold_test_taskB_emoji.txt'\n",
    "    corpus, _ = parse_dataset(dataset)\n",
    "    corpus_preprocessed = json.load(open('../extra_resources/test_preprocessed.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    corpus_preprocessed = json.load(open('../extra_resources/train_preprocessed.txt','r'))\n",
    "else:\n",
    "    corpus_preprocessed = json.load(open('../extra_resources/test_preprocessed.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkIt(seq, n):\n",
    "    \"\"\"splits the list into n approximately equal sub-lists. source: goo.gl/VrHKeR\"\"\"\n",
    "    avg = len(seq) / float(n)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# intensity features - 3 binarized features for splitted tweets which show: \n",
    "# 1) the intensity of the left half\n",
    "# 2) the intensity of the right half\n",
    "# 3) the difference between the polarities of left and right halves \n",
    "\n",
    "# to run this, first download stanford corenlp\n",
    "# install pycorenlp: pip install pycorenlp\n",
    "# then enter this on terminal, within the diretcory of corenlp \n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"sentiment\" -port 9000 -timeout 30000\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "feats_1 = []\n",
    "\n",
    "for text in corpus:\n",
    "    part1, part2 = chunkIt(text, 2)\n",
    "    output1 = int(nlp.annotate(part1, properties={\n",
    "                           'annotators': 'sentiment',\n",
    "                             'outputFormat': 'json'\n",
    "                           })['sentences'][0]['sentimentValue'])\n",
    "    output2 = int(nlp.annotate(part2, properties={\n",
    "                           'annotators': 'sentiment',\n",
    "                             'outputFormat': 'json'\n",
    "                           })['sentences'][0]['sentimentValue'])    \n",
    "    leftIntensity = rightIntensity = polarityDiff = 0\n",
    "    if output1 in [0,4]:\n",
    "        leftIntensity = 1\n",
    "    if output2 in [0,4]:\n",
    "        rightIntensity = 1\n",
    "    if (output1>2 and output2<2) or (output1<2 and output2>2):\n",
    "        polarityDiff = 1\n",
    "    feats_1.append(np.array([leftIntensity, rightIntensity, polarityDiff]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast \n",
    "df = pd.read_csv('../extra_resources/Emoji_Sentiment_Data_v1.0.csv')\n",
    "\n",
    "df = df[['Emoji', 'Negative', 'Neutral', 'Positive']]\n",
    "tuples = [tuple(x) for x in df.values]\n",
    "# tuples are of the form (emoji, negative, neutral, positive)\n",
    "\n",
    "idx2lb = {0:-1, 1:0, 2:1}\n",
    "emoji_sentimens = {}\n",
    "for val in tuples:\n",
    "    emoji_sentimens[val[0]] = idx2lb[np.argmax(np.array(val[1:]))]\n",
    "\n",
    "def extractEmoticon(tweet):\n",
    "    \"\"\"returns all the emoticons in tweet\"\"\"\n",
    "    return re.findall(r'[\\U0001f600-\\U0001f650]', ' '.join(tweet))\n",
    "\n",
    "twts = [extractEmoticon(twt[0]) for twt in corpus_preprocessed]\n",
    "twts = [[emoji_sentimens[emoji] for emoji in twt] for twt in twts]\n",
    "\n",
    "def extractHashtag(tweet):\n",
    "    t = tweet.split(' ')\n",
    "    text = []\n",
    "    hashtagText = []\n",
    "    oneHashtag = []\n",
    "    flag = 0\n",
    "    for w in t:\n",
    "        if w == \"<hashtag>\":\n",
    "            flag = 1\n",
    "            continue\n",
    "        if flag == 1:\n",
    "            if w == \"</hashtag>\":\n",
    "                hashtagText.append(oneHashtag)\n",
    "                oneHashtag = []\n",
    "                flag = 0\n",
    "            else:\n",
    "                oneHashtag.append(w)\n",
    "        else:\n",
    "            text.append(w)\n",
    "    return text, hashtagText\n",
    "\n",
    "txt = [extractHashtag(tweet) for tweet in corpus_preprocessed]\n",
    "\n",
    "assert len(txt) == len(twts)\n",
    "txt = [(txt[i][0], txt[i][1], twts[i]) for i in range(len(twts))]\n",
    "\n",
    "# 0: very negative\n",
    "# 1: negative \n",
    "# 2: neutral \n",
    "# 3: positive \n",
    "# 4: very positive \n",
    "    \n",
    "def sentiment(txt):\n",
    "    \"\"\"compute sentiment for text\"\"\"\n",
    "    txt = ' '.join(txt)\n",
    "    if not len(txt): return 2\n",
    "    output = int(nlp.annotate(txt, properties={\n",
    "                              'annotators': 'tokenize,ssplit,pos,depparse,parse,sentiment',\n",
    "                                'outputFormat': 'json'\n",
    "                              })['sentences'][0]['sentimentValue'])\n",
    "    return output\n",
    "\n",
    "def contrast(twt):\n",
    "    \"\"\"search for emotion contrast in hastag, emoticon and tweet text\"\"\"\n",
    "    contrast = 0 # contrast flag\n",
    "    txt_sentiment = sentiment(twt[0])\n",
    "    htag_sentiment = [sentiment(h) for hash_segments in twt[1] for h in hash_segments]\n",
    "    emoji_sentiment = twt[2]\n",
    "\n",
    "    if (txt_sentiment in {2,3,4}) and (set(htag_sentiment) & {0,1}):\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {0,1}) and (set(htag_sentiment) & {3,4}): # maybe later try with adding 2\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {2,3,4}) and (set(emoji_sentiment) & {-1}):\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {0,1}) and (set(emoji_sentiment) & {1}):\n",
    "        contrast = 1\n",
    "    elif {-1,1} in set(emoji_sentiment):\n",
    "        contrast = 1\n",
    "    elif ({0,4} in set(htag_sentiment)) or ({0,3} in set(htag_sentiment)) or ({1,4} in set(htag_sentiment)):\n",
    "        contrast = 1\n",
    "    elif (set(htag_sentiment) & {0,1}) and (set(emoji_sentiment) & {1}):\n",
    "        contrast = 1\n",
    "    elif (set(htag_sentiment) & {3,4}) and (set(emoji_sentiment) & {-1}):\n",
    "        contrast = 1\n",
    "    return contrast\n",
    "\n",
    "contrast_feats = [np.array([contrast(twt)]) for twt in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ekphrasis-based features (extracted from pre-processed data)  2*24 features\n",
    "\n",
    "tags =  ['<allcaps>', '<annoyed>', '<censored>', '<date>', '<elongated>', '<emphasis>', '<happy>',\n",
    "         '<hashtag>', '<heart>', '<kiss>', '<laugh>', '<money>', '<number>', '<percent>', '<phone>',\n",
    "         '<repeated>', '<sad>', '<shocking>', '<surprise>', '<time>', '<tong>', '<url>', '<user>',\n",
    "         '<wink>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_vecs(twt, n=2):\n",
    "    \"\"\"extract a feature vector for a single tweet, based on the counts of the annotation tags\n",
    "        split the tweet to n equal parts and computes the same features for each part\"\"\"\n",
    "    twt = twt.split()\n",
    "    chunks = chunkIt(twt, n)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for tag in tags:\n",
    "            scores.append(sum(1 for t in chunk if t == tag))\n",
    "    return scores\n",
    "    \n",
    "\n",
    "def feats(text):\n",
    "    \"\"\"apply the tweet_vecs function on all tweets and return a result in a list\"\"\"\n",
    "    return [tweet_vecs(twt) for twt in text]\n",
    "\n",
    "ekphrasis_feats = [np.array(v) for v in feats(corpus_preprocessed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity features, which didn't result in good perfoemance and hence, were discarded\n",
    "\n",
    "from ekphrasis.utils.nlp import polarity\n",
    "polarity_flag = False\n",
    "\n",
    "polarity_vectors = []\n",
    "for tweet in corpus_preprocessed:\n",
    "    chunks = chunkIt(tweet, 2)\n",
    "    polarity_vectors.append(np.concatenate(((polarity(chunks[0])[1], polarity(chunks[1])[1])), axis=0))\n",
    "\n",
    "assert len(ekphrasis_feats) == len(polarity_vectors)\n",
    "\n",
    "if polarity_flag: \n",
    "    ekphrasis_feats = [np.concatenate((ekphrasis_feats[i], polarity_vectors[i])) for i in range(len(ekphrasis_feats))]\n",
    "\n",
    "# np.save('ekphrasis_feats', np.array(ekphrasis_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no-chunk (i.e. whole tweet) ekphrasis feats \n",
    "def tweet_vecs(twt):\n",
    "    words = twt.split()    \n",
    "    scores = []\n",
    "    \n",
    "    for tag in tags:\n",
    "        scores.append(sum(1 for word in words if word == tag))\n",
    "    return scores\n",
    "    \n",
    "\n",
    "def feats(text):\n",
    "    \"\"\"apply the tweet_vecs function on all tweets and return a result in a list\"\"\"\n",
    "    return [tweet_vecs(twt) for twt in text]\n",
    "\n",
    "ekphrasis_feats_no_chunks = [np.array(v) for v in feats(corpus_preprocessed)]\n",
    "# np.save('ekphrasis_feats_no_chunks', np.array(ekphrasis_feats_no_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vader feats \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "maxlength = max([len(s) for s in corpus_preprocessed])\n",
    "\n",
    "def senti_patterns(text):\n",
    "    patterns = []\n",
    "    \"\"\"receives list of list of tokens (sentences), returns rise(R) and fall(F) patterns in sentiment scores\"\"\"\n",
    "    for sentence in text:\n",
    "        sent_pattern = []\n",
    "        for word in sentence:\n",
    "            senti = vader.polarity_scores(word)['compound']\n",
    "            if senti > 0:\n",
    "                sent_pattern.append('R')\n",
    "            elif senti < 0:\n",
    "                sent_pattern.append('F')\n",
    "        sent_pattern = [x[0] for x in groupby(sent_pattern)]\n",
    "        if not sent_pattern: sent_pattern.append('S') \n",
    "        patterns.append(sent_pattern)\n",
    "    return patterns\n",
    "\n",
    "pat = senti_patterns(corpus_preprocessed)\n",
    "\n",
    "pats_with_bigrams = []\n",
    "for tags in pat:\n",
    "    bigrams = list(zip(tags, tags[1:]))\n",
    "    tags.extend(bigrams)\n",
    "    pats_with_bigrams.append(tags)\n",
    "\n",
    "vader_vecs = [] \n",
    "\n",
    "# each count vector is a 5-diemnsional numpy array denoting the counts in the following order:\n",
    "############################# ['S', 'R', 'F', ('R', 'F'), ('F', 'R')] ######################\n",
    "\n",
    "for pat in pats_with_bigrams:\n",
    "    counts = Counter(pat)\n",
    "    vader_vecs.append(np.array([counts['S'],counts['R'],counts['F'],counts[('R', 'F')], counts[('F', 'R')]]))\n",
    "    \n",
    "# np.save('test_vader_feats.npy', np.array(vader_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load new Topic Modeling features\n",
    "if TRAIN:\n",
    "    topicFeats_2chunks = np.load('./topic_feats_10_2chunks.npy')\n",
    "    topicFeats_no_chunks = np.load('./topic_feats_10_no_chunks.npy')\n",
    "else:\n",
    "    topicFeats_2chunks = np.load('./test_topic_feats_10_2chunks.npy')\n",
    "    topicFeats_no_chunks = np.load('./test_topic_feats_10_no_chunks.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concatenate all the features \n",
    "features = np.concatenate((feats_1, \n",
    "                           contrast_feats, \n",
    "                           ekphrasis_feats, \n",
    "                           ekphrasis_feats_no_chunks, \n",
    "                           vader_vecs,\n",
    "                           topicFeats_2chunks,\n",
    "                           topicFeats_no_chunks), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    np.save('all_train_feats_TaskB.npy', features)\n",
    "else:\n",
    "    np.save('all_test_feats_TaskB.npy', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
