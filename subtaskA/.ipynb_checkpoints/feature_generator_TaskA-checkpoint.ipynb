{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script generates the features for classification in subtask A. Set the TRAIN flag to True to \n",
    "    generate features for the training data, and to False to generate features for test data\"\"\"\n",
    "\n",
    "import json, re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "from load import parse_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    dataset='../datasets/train/SemEval2018-T3-train-taskA_emoji.txt'\n",
    "    corpus, _ = parse_dataset(dataset)\n",
    "    corpus_preprocessed = json.load(open('../extra_resources/train_preprocessed.txt','r'))\n",
    "else:\n",
    "    dataset='../datasets/test_TaskA/SemEval2018-T3_input_test_taskA_emoji.txt'\n",
    "    corpus = parse_dataset(dataset)\n",
    "    corpus_preprocessed = json.load(open('../extra_resources/test_preprocessed.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intensity features - 3 binarized features for splitted tweets which show: \n",
    "# 1) the intensity of the left half\n",
    "# 2) the intensity of the right half\n",
    "# 3) the difference between the polarities of left and right halves \n",
    "\n",
    "# to run this, first download stanford corenlp\n",
    "# install pycorenlp: pip install pycorenlp\n",
    "# then enter this on terminal, within the diretcory of corenlp \n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"sentiment\" -port 9000 -timeout 30000\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "def chunkIt(seq, n):\n",
    "    \"\"\"splits the list into n approximately equal sub-lists. source: goo.gl/VrHKeR\"\"\"\n",
    "    avg = len(seq) / float(n)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    return out\n",
    "\n",
    "feats_1 = []\n",
    "\n",
    "for text in corpus:\n",
    "    part1, part2 = chunkIt(text, 2)\n",
    "    output1 = int(nlp.annotate(part1, properties={\n",
    "                           'annotators': 'sentiment',\n",
    "                             'outputFormat': 'json'\n",
    "                           })['sentences'][0]['sentimentValue'])\n",
    "    output2 = int(nlp.annotate(part2, properties={\n",
    "                           'annotators': 'sentiment',\n",
    "                             'outputFormat': 'json'\n",
    "                           })['sentences'][0]['sentimentValue'])    \n",
    "    leftIntensity = rightIntensity = polarityDiff = 0\n",
    "    if output1 in [0,4]:\n",
    "        leftIntensity = 1\n",
    "    if output2 in [0,4]:\n",
    "        rightIntensity = 1\n",
    "    if (output1>2 and output2<2) or (output1<2 and output2>2):\n",
    "        polarityDiff = 1\n",
    "    feats_1.append(np.array([leftIntensity, rightIntensity, polarityDiff]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast\n",
    "df = pd.read_csv('../extra_resources/Emoji_Sentiment_Data_v1.0.csv')\n",
    "\n",
    "df = df[['Emoji', 'Negative', 'Neutral', 'Positive']]\n",
    "tuples = [tuple(x) for x in df.values]\n",
    "# tuples are of the form (emoji, negative, neutral, positive)\n",
    "\n",
    "idx2lb = {0:-1, 1:0, 2:1}\n",
    "emoji_sentimens = {}\n",
    "for val in tuples:\n",
    "    emoji_sentimens[val[0]] = idx2lb[np.argmax(np.array(val[1:]))]\n",
    "\n",
    "def extractEmoticon(tweet):\n",
    "    \"\"\"returns all the emoticons in tweet\"\"\"\n",
    "    return re.findall(r'[\\U0001f600-\\U0001f650]', ' '.join(tweet))\n",
    "\n",
    "twts = [extractEmoticon(twt[0]) for twt in corpus_preprocessed]\n",
    "twts = [[emoji_sentimens[emoji] for emoji in twt] for twt in twts]\n",
    "\n",
    "def extractHashtag(tweet):\n",
    "    t = tweet.split(' ')\n",
    "    text = []\n",
    "    hashtagText = []\n",
    "    oneHashtag = []\n",
    "    flag = 0\n",
    "    for w in t:\n",
    "        if w == \"<hashtag>\":\n",
    "            flag = 1\n",
    "            continue\n",
    "        if flag == 1:\n",
    "            if w == \"</hashtag>\":\n",
    "                hashtagText.append(oneHashtag)\n",
    "                oneHashtag = []\n",
    "                flag = 0\n",
    "            else:\n",
    "                oneHashtag.append(w)\n",
    "        else:\n",
    "            text.append(w)\n",
    "    return text, hashtagText\n",
    "\n",
    "txt = [extractHashtag(tweet) for tweet in corpus_preprocessed]\n",
    "\n",
    "assert len(txt) == len(twts)\n",
    "txt = [(txt[i][0], txt[i][1], twts[i]) for i in range(len(twts))]\n",
    "\n",
    "# 0: very negative\n",
    "# 1: negative \n",
    "# 2: neutral \n",
    "# 3: positive \n",
    "# 4: very positive \n",
    "    \n",
    "def sentiment(txt):\n",
    "    \"\"\"compute sentiment for text\"\"\"\n",
    "    txt = ' '.join(txt)\n",
    "    if not len(txt): return 2\n",
    "    output = int(nlp.annotate(txt, properties={\n",
    "                              'annotators': 'tokenize,ssplit,pos,depparse,parse,sentiment',\n",
    "                                'outputFormat': 'json'\n",
    "                              })['sentences'][0]['sentimentValue'])\n",
    "    return output\n",
    "\n",
    "def contrast(twt):\n",
    "    \"\"\"search for emotion contrast in hastag, emoticon and tweet text\"\"\"\n",
    "    contrast = 0 # contrast flag\n",
    "    txt_sentiment = sentiment(twt[0])\n",
    "    htag_sentiment = [sentiment(h) for hash_segments in twt[1] for h in hash_segments]\n",
    "    emoji_sentiment = twt[2]\n",
    "\n",
    "    if (txt_sentiment in {2,3,4}) and (set(htag_sentiment) & {0,1}):\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {0,1}) and (set(htag_sentiment) & {3,4}): # maybe later try adding 2\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {2,3,4}) and (set(emoji_sentiment) & {-1}):\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {0,1}) and (set(emoji_sentiment) & {1}):\n",
    "        contrast = 1\n",
    "    elif {-1,1} in set(emoji_sentiment):\n",
    "        contrast = 1\n",
    "    elif ({0,4} in set(htag_sentiment)) or ({0,3} in set(htag_sentiment)) or ({1,4} in set(htag_sentiment)):\n",
    "        contrast = 1\n",
    "    elif (set(htag_sentiment) & {0,1}) and (set(emoji_sentiment) & {1}):\n",
    "        contrast = 1\n",
    "    elif (set(htag_sentiment) & {3,4}) and (set(emoji_sentiment) & {-1}):\n",
    "        contrast = 1\n",
    "    return contrast\n",
    "\n",
    "\n",
    "contrast_feats = [np.array([contrast(twt)]) for twt in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats = [feats[i].append(contrast_feats[i]) for i in range(len(feats))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ekphrasis-based features (extracted from pre-processed data)\n",
    "\n",
    "tags =  ['<allcaps>', '<annoyed>', '<censored>', '<date>', '<elongated>', '<emphasis>', '<happy>',\n",
    "         '<hashtag>', '<heart>', '<kiss>', '<laugh>', '<money>', '<number>', '<percent>', '<phone>',\n",
    "         '<repeated>', '<sad>', '<shocking>', '<surprise>', '<time>', '<tong>', '<url>', '<user>',\n",
    "         '<wink>']\n",
    "\n",
    "def tweet_vecs(twt, n=2):\n",
    "    \"\"\"extract a feature vector for a single tweet, based on the counts of the annotation tags\n",
    "        split the tweet to n equal parts and computes the same features for each part\"\"\"\n",
    "    twt = twt.split()\n",
    "    chunks = chunkIt(twt, n)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for tag in tags:\n",
    "            scores.append(sum(1 for t in chunk if t == tag))\n",
    "    return scores\n",
    "    \n",
    "def feats(text):\n",
    "    \"\"\"apply the tweet_vecs function on all tweets and return a result in a list\"\"\"\n",
    "    return [tweet_vecs(twt) for twt in text]\n",
    "\n",
    "ekphrasis_feats = [np.array(v) for v in feats(corpus_preprocessed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.utils.nlp import polarity\n",
    "polarity_flag = True\n",
    "\n",
    "polarity_vectors = []\n",
    "for tweet in corpus_preprocessed:\n",
    "    chunks = chunkIt(tweet, 2)\n",
    "    polarity_vectors.append(np.concatenate(((polarity(chunks[0])[1], polarity(chunks[1])[1])), axis=0))\n",
    "\n",
    "assert len(ekphrasis_feats) == len(polarity_vectors)\n",
    "\n",
    "if polarity_flag: \n",
    "    ekphrasis_feats = [np.concatenate((ekphrasis_feats[i], polarity_vectors[i])) for i in range(len(ekphrasis_feats))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekphrasis_feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features \n",
    "features = np.concatenate((feats_1, contrast_feats, ekphrasis_feats), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the features in a numpy file \n",
    "if TRAIN:\n",
    "    np.save('train_feats_taskA.npy', features)\n",
    "else:\n",
    "    np.save('test_feats_taskA.npy', features)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
