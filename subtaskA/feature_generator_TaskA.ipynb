{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script generates the features for classification in subtask A. Set the TRAIN flag to True to \n",
    "    generate features for the training data, and to False to generate features for test data\"\"\"\n",
    "\n",
    "import json, re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "from load import parse_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    dataset='../datasets/train/SemEval2018-T3-train-taskA_emoji.txt'\n",
    "    corpus, _ = parse_dataset(dataset)\n",
    "    corpus_preprocessed = json.load(open('../extra_resources/train_preprocessed.txt','r'))\n",
    "else:\n",
    "    dataset='../datasets/test_TaskA/SemEval2018-T3_input_test_taskA_emoji.txt'\n",
    "    corpus = parse_dataset(dataset)\n",
    "    corpus_preprocessed = json.load(open('../extra_resources/test_preprocessed.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences': [{'index': 0, 'parse': '(ROOT\\n  (NP\\n    (NP (JJ Sweet) (NNP United) (NNPS Nations))\\n    (NP (NN video))\\n    (. .)))', 'basicDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'Nations'}, {'dep': 'amod', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 1, 'dependentGloss': 'Sweet'}, {'dep': 'compound', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 2, 'dependentGloss': 'United'}, {'dep': 'dep', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 4, 'dependentGloss': 'video'}, {'dep': 'punct', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 5, 'dependentGloss': '.'}], 'enhancedDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'Nations'}, {'dep': 'amod', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 1, 'dependentGloss': 'Sweet'}, {'dep': 'compound', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 2, 'dependentGloss': 'United'}, {'dep': 'dep', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 4, 'dependentGloss': 'video'}, {'dep': 'punct', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 5, 'dependentGloss': '.'}], 'enhancedPlusPlusDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'Nations'}, {'dep': 'amod', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 1, 'dependentGloss': 'Sweet'}, {'dep': 'compound', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 2, 'dependentGloss': 'United'}, {'dep': 'dep', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 4, 'dependentGloss': 'video'}, {'dep': 'punct', 'governor': 3, 'governorGloss': 'Nations', 'dependent': 5, 'dependentGloss': '.'}], 'sentimentValue': '2', 'sentiment': 'Neutral', 'sentimentDistribution': [0.03179968149876, 0.23486607637778, 0.61393046798584, 0.10813861797897, 0.01126515615866], 'sentimentTree': '(ROOT|sentiment=2|prob=0.614\\n  (@NP|sentiment=2|prob=0.689\\n    (NP|sentiment=2|prob=0.939 (JJ|sentiment=3|prob=0.961 Sweet)\\n      (@NP|sentiment=2|prob=0.646 (NNP|sentiment=2|prob=0.980 United) (NNPS|sentiment=2|prob=0.631 Nations)))\\n    (NP|sentiment=2|prob=0.994 video))\\n  (.|sentiment=2|prob=0.997 .))', 'tokens': [{'index': 1, 'word': 'Sweet', 'originalText': 'Sweet', 'characterOffsetBegin': 0, 'characterOffsetEnd': 5, 'pos': 'JJ', 'before': '', 'after': ' '}, {'index': 2, 'word': 'United', 'originalText': 'United', 'characterOffsetBegin': 6, 'characterOffsetEnd': 12, 'pos': 'NNP', 'before': ' ', 'after': ' '}, {'index': 3, 'word': 'Nations', 'originalText': 'Nations', 'characterOffsetBegin': 13, 'characterOffsetEnd': 20, 'pos': 'NNPS', 'before': ' ', 'after': ' '}, {'index': 4, 'word': 'video', 'originalText': 'video', 'characterOffsetBegin': 21, 'characterOffsetEnd': 26, 'pos': 'NN', 'before': ' ', 'after': ''}, {'index': 5, 'word': '.', 'originalText': '.', 'characterOffsetBegin': 26, 'characterOffsetEnd': 27, 'pos': '.', 'before': '', 'after': ' '}]}, {'index': 1, 'parse': '(ROOT\\n  (PP (RB Just) (IN in)\\n    (NP\\n      (NP (NN time))\\n      (PP (IN for)\\n        (NP (NNP Chris))))))', 'basicDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'time'}, {'dep': 'advmod', 'governor': 3, 'governorGloss': 'time', 'dependent': 1, 'dependentGloss': 'Just'}, {'dep': 'case', 'governor': 3, 'governorGloss': 'time', 'dependent': 2, 'dependentGloss': 'in'}, {'dep': 'case', 'governor': 5, 'governorGloss': 'Chris', 'dependent': 4, 'dependentGloss': 'for'}, {'dep': 'nmod', 'governor': 3, 'governorGloss': 'time', 'dependent': 5, 'dependentGloss': 'Chris'}], 'enhancedDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'time'}, {'dep': 'advmod', 'governor': 3, 'governorGloss': 'time', 'dependent': 1, 'dependentGloss': 'Just'}, {'dep': 'case', 'governor': 3, 'governorGloss': 'time', 'dependent': 2, 'dependentGloss': 'in'}, {'dep': 'case', 'governor': 5, 'governorGloss': 'Chris', 'dependent': 4, 'dependentGloss': 'for'}, {'dep': 'nmod:for', 'governor': 3, 'governorGloss': 'time', 'dependent': 5, 'dependentGloss': 'Chris'}], 'enhancedPlusPlusDependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 3, 'dependentGloss': 'time'}, {'dep': 'advmod', 'governor': 3, 'governorGloss': 'time', 'dependent': 1, 'dependentGloss': 'Just'}, {'dep': 'case', 'governor': 3, 'governorGloss': 'time', 'dependent': 2, 'dependentGloss': 'in'}, {'dep': 'case', 'governor': 5, 'governorGloss': 'Chris', 'dependent': 4, 'dependentGloss': 'for'}, {'dep': 'nmod:for', 'governor': 3, 'governorGloss': 'time', 'dependent': 5, 'dependentGloss': 'Chris'}], 'sentimentValue': '2', 'sentiment': 'Neutral', 'sentimentDistribution': [0.00489744500437, 0.04784626147481, 0.84605668256447, 0.09679048371208, 0.00440912724428], 'sentimentTree': '(ROOT|sentiment=2|prob=0.846 (RB|sentiment=2|prob=0.997 Just)\\n  (@PP|sentiment=2|prob=0.913 (IN|sentiment=2|prob=0.993 in)\\n    (NP|sentiment=2|prob=0.906 (NP|sentiment=2|prob=0.999 time)\\n      (PP|sentiment=2|prob=0.904 (IN|sentiment=2|prob=0.992 for) (NP|sentiment=3|prob=0.961 Chris)))))', 'tokens': [{'index': 1, 'word': 'Just', 'originalText': 'Just', 'characterOffsetBegin': 28, 'characterOffsetEnd': 32, 'pos': 'RB', 'before': ' ', 'after': ' '}, {'index': 2, 'word': 'in', 'originalText': 'in', 'characterOffsetBegin': 33, 'characterOffsetEnd': 35, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 3, 'word': 'time', 'originalText': 'time', 'characterOffsetBegin': 36, 'characterOffsetEnd': 40, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 4, 'word': 'for', 'originalText': 'for', 'characterOffsetBegin': 41, 'characterOffsetEnd': 44, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 5, 'word': 'Chris', 'originalText': 'Chris', 'characterOffsetBegin': 45, 'characterOffsetEnd': 50, 'pos': 'NNP', 'before': ' ', 'after': ''}]}]}\n"
     ]
    }
   ],
   "source": [
    "# intensity features - 3 binarized features for splitted tweets which show: \n",
    "# 1) the intensity of the left half\n",
    "# 2) the intensity of the right half\n",
    "# 3) the difference between the polarities of left and right halves \n",
    "\n",
    "# to run this, first download stanford corenlp\n",
    "# install pycorenlp: pip install pycorenlp\n",
    "# then enter this on terminal, within the diretcory of corenlp \n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"sentiment\" -port 9000 -timeout 30000\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "def chunkIt(seq, n):\n",
    "    \"\"\"splits the list into n approximately equal sub-lists. source: goo.gl/VrHKeR\"\"\"\n",
    "    avg = len(seq) / float(n)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    return out\n",
    "\n",
    "feats_1 = []\n",
    "\n",
    "#for text in corpus:\n",
    "#    part1, part2 = chunkIt(text, 2)\n",
    "    \n",
    "part1, part2 = chunkIt(corpus[0], 2)\n",
    "\n",
    "output1 = nlp.annotate(part1, properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                         'outputFormat': 'json'\n",
    "                       })\n",
    "                 #)['sentences'][0]['sentimentValue'])\n",
    "output2 = int(nlp.annotate(part2, properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                         'outputFormat': 'json'\n",
    "                       })['sentences'][0]['sentimentValue'])\n",
    "print(output1)\n",
    "    #leftIntensity = rightIntensity = polarityDiff = 0\n",
    "    #if output1 in [0,4]:\n",
    "    #    leftIntensity = 1\n",
    "    #if output2 in [0,4]:\n",
    "    #    rightIntensity = 1\n",
    "    #if (output1>2 and output2<2) or (output1<2 and output2>2):\n",
    "    #    polarityDiff = 1\n",
    "    #feats_1.append(np.array([leftIntensity, rightIntensity, polarityDiff]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast\n",
    "df = pd.read_csv('../extra_resources/Emoji_Sentiment_Data_v1.0.csv')\n",
    "\n",
    "df = df[['Emoji', 'Negative', 'Neutral', 'Positive']]\n",
    "tuples = [tuple(x) for x in df.values]\n",
    "# tuples are of the form (emoji, negative, neutral, positive)\n",
    "\n",
    "idx2lb = {0:-1, 1:0, 2:1}\n",
    "emoji_sentimens = {}\n",
    "for val in tuples:\n",
    "    emoji_sentimens[val[0]] = idx2lb[np.argmax(np.array(val[1:]))]\n",
    "\n",
    "def extractEmoticon(tweet):\n",
    "    \"\"\"returns all the emoticons in tweet\"\"\"\n",
    "    return re.findall(r'[\\U0001f600-\\U0001f650]', ' '.join(tweet))\n",
    "\n",
    "twts = [extractEmoticon(twt[0]) for twt in corpus_preprocessed]\n",
    "twts = [[emoji_sentimens[emoji] for emoji in twt] for twt in twts]\n",
    "\n",
    "def extractHashtag(tweet):\n",
    "    t = tweet.split(' ')\n",
    "    text = []\n",
    "    hashtagText = []\n",
    "    oneHashtag = []\n",
    "    flag = 0\n",
    "    for w in t:\n",
    "        if w == \"<hashtag>\":\n",
    "            flag = 1\n",
    "            continue\n",
    "        if flag == 1:\n",
    "            if w == \"</hashtag>\":\n",
    "                hashtagText.append(oneHashtag)\n",
    "                oneHashtag = []\n",
    "                flag = 0\n",
    "            else:\n",
    "                oneHashtag.append(w)\n",
    "        else:\n",
    "            text.append(w)\n",
    "    return text, hashtagText\n",
    "\n",
    "txt = [extractHashtag(tweet) for tweet in corpus_preprocessed]\n",
    "\n",
    "assert len(txt) == len(twts)\n",
    "txt = [(txt[i][0], txt[i][1], twts[i]) for i in range(len(twts))]\n",
    "\n",
    "# 0: very negative\n",
    "# 1: negative \n",
    "# 2: neutral \n",
    "# 3: positive \n",
    "# 4: very positive \n",
    "    \n",
    "def sentiment(txt):\n",
    "    \"\"\"compute sentiment for text\"\"\"\n",
    "    txt = ' '.join(txt)\n",
    "    if not len(txt): return 2\n",
    "    output = int(nlp.annotate(txt, properties={\n",
    "                              'annotators': 'tokenize,ssplit,pos,depparse,parse,sentiment',\n",
    "                                'outputFormat': 'json'\n",
    "                              })['sentences'][0]['sentimentValue'])\n",
    "    return output\n",
    "\n",
    "def contrast(twt):\n",
    "    \"\"\"search for emotion contrast in hastag, emoticon and tweet text\"\"\"\n",
    "    contrast = 0 # contrast flag\n",
    "    txt_sentiment = sentiment(twt[0])\n",
    "    htag_sentiment = [sentiment(h) for hash_segments in twt[1] for h in hash_segments]\n",
    "    emoji_sentiment = twt[2]\n",
    "\n",
    "    if (txt_sentiment in {2,3,4}) and (set(htag_sentiment) & {0,1}):\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {0,1}) and (set(htag_sentiment) & {3,4}): # maybe later try adding 2\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {2,3,4}) and (set(emoji_sentiment) & {-1}):\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {0,1}) and (set(emoji_sentiment) & {1}):\n",
    "        contrast = 1\n",
    "    elif {-1,1} in set(emoji_sentiment):\n",
    "        contrast = 1\n",
    "    elif ({0,4} in set(htag_sentiment)) or ({0,3} in set(htag_sentiment)) or ({1,4} in set(htag_sentiment)):\n",
    "        contrast = 1\n",
    "    elif (set(htag_sentiment) & {0,1}) and (set(emoji_sentiment) & {1}):\n",
    "        contrast = 1\n",
    "    elif (set(htag_sentiment) & {3,4}) and (set(emoji_sentiment) & {-1}):\n",
    "        contrast = 1\n",
    "    return contrast\n",
    "\n",
    "\n",
    "contrast_feats = [np.array([contrast(twt)]) for twt in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats = [feats[i].append(contrast_feats[i]) for i in range(len(feats))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ekphrasis-based features (extracted from pre-processed data)\n",
    "\n",
    "tags =  ['<allcaps>', '<annoyed>', '<censored>', '<date>', '<elongated>', '<emphasis>', '<happy>',\n",
    "         '<hashtag>', '<heart>', '<kiss>', '<laugh>', '<money>', '<number>', '<percent>', '<phone>',\n",
    "         '<repeated>', '<sad>', '<shocking>', '<surprise>', '<time>', '<tong>', '<url>', '<user>',\n",
    "         '<wink>']\n",
    "\n",
    "def tweet_vecs(twt, n=2):\n",
    "    \"\"\"extract a feature vector for a single tweet, based on the counts of the annotation tags\n",
    "        split the tweet to n equal parts and computes the same features for each part\"\"\"\n",
    "    twt = twt.split()\n",
    "    chunks = chunkIt(twt, n)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for tag in tags:\n",
    "            scores.append(sum(1 for t in chunk if t == tag))\n",
    "    return scores\n",
    "    \n",
    "def feats(text):\n",
    "    \"\"\"apply the tweet_vecs function on all tweets and return a result in a list\"\"\"\n",
    "    return [tweet_vecs(twt) for twt in text]\n",
    "\n",
    "ekphrasis_feats = [np.array(v) for v in feats(corpus_preprocessed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.utils.nlp import polarity\n",
    "\n",
    "polarity_flag = True\n",
    "\n",
    "polarity_vectors = []\n",
    "for tweet in corpus_preprocessed:\n",
    "    chunks = chunkIt(tweet, 2)\n",
    "    polarity_vectors.append(np.concatenate(((polarity(chunks[0])[1], polarity(chunks[1])[1])), axis=0))\n",
    "\n",
    "assert len(ekphrasis_feats) == len(polarity_vectors)\n",
    "\n",
    "if polarity_flag: \n",
    "    ekphrasis_feats = [np.concatenate((ekphrasis_feats[i], polarity_vectors[i])) for i in range(len(ekphrasis_feats))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ekphrasis.utils.nlp import polarity\n",
    "\n",
    "#doc = 'As I was walking though a very bad neighbourhood I noticed there were a lot of nice and friendly people wanting to help me be safe. I was suprised and felt good'\n",
    "#doc = doc.split()\n",
    "\n",
    "#print(polarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ekphrasis_feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features \n",
    "features = np.concatenate((feats_1, contrast_feats, ekphrasis_feats), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3834"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3834, 58)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the features in a numpy file \n",
    "if TRAIN:\n",
    "    np.save('train_feats_taskA.npy', features)\n",
    "else:\n",
    "    np.save('test_feats_taskA.npy', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
